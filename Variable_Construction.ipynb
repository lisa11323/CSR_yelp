{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JDFKoHs9t51f",
        "6gHvAo_TWSNk",
        "IRR8V_u0cnJy",
        "Q67X1QKfdJPW",
        "hIw_BzsVePY-",
        "KGs-nO1mc7D_",
        "PPFbjcSvk4T9"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisa11323/CSR_yelp/blob/main/Variable_Construction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Prepare dataset"
      ],
      "metadata": {
        "id": "JDFKoHs9t51f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download open dataset\n",
        "# https://business.yelp.com/data/resources/open-dataset/"
      ],
      "metadata": {
        "id": "dFUGfs8nuhpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload files manually from local machine\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "uWHlB-TeWb3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert JSON to CSV\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load Yelp dataset JSON files\n",
        "df_review = pd.read_json(\"yelp_academic_dataset_review.json\", lines=True)\n",
        "df_user = pd.read_json(\"yelp_academic_dataset_user.json\", lines=True)\n",
        "df_biz = pd.read_json(\"yelp_academic_dataset_business.json\", lines=True)\n",
        "\n",
        "# Save to CSV\n",
        "df_review.to_csv(\"review.csv\", index=False)\n",
        "df_user.to_csv(\"user.csv\", index=False)\n",
        "df_biz.to_csv(\"business.csv\", index=False)"
      ],
      "metadata": {
        "id": "PXdz0KCfXEvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download CSV files to local machine\n",
        "files.download(\"yelp_academic_dataset_review.csv\")\n",
        "files.download(\"yelp_academic_dataset_user.csv\")\n",
        "files.download(\"yelp_academic_dataset_business.csv\")"
      ],
      "metadata": {
        "id": "2s5TvENKXKa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Sampling Dataset"
      ],
      "metadata": {
        "id": "6gHvAo_TWSNk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNaSZvJqWISv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Filter Health & Medical businesses\n",
        "hm_biz = df_biz[df_biz['categories'].str.contains(\"Health & Medical\", na=False)]\n",
        "hm_business_ids = hm_biz['business_id'].tolist()\n",
        "\n",
        "# Select reviews for Health & Medical businesses (2020â€“2022)\n",
        "df_hm = df_review[df_review['business_id'].isin(hm_business_ids)].copy()\n",
        "df_hm['date'] = pd.to_datetime(df_hm['date'])\n",
        "df_hm = df_hm[(df_hm['date'].dt.year >= 2020) & (df_hm['date'].dt.year <= 2022)]\n",
        "\n",
        "def preprocess_reviews(df, df_user):\n",
        "    # Keep review-level funny/cool with prefix\n",
        "    df = df.rename(columns={\n",
        "        'useful': 'rev_useful',\n",
        "        'funny': 'rev_funny',\n",
        "        'cool': 'rev_cool'\n",
        "    })\n",
        "    df = df.drop(columns=['business_id'], errors='ignore')\n",
        "\n",
        "    # Select user-level columns including funny/cool\n",
        "    user_columns = ['user_id', 'name', 'review_count', 'yelping_since', 'useful', 'elite',\n",
        "                    'friends', 'fans', 'average_stars', 'compliment_hot', 'compliment_more',\n",
        "                    'compliment_profile', 'compliment_cute', 'compliment_list',\n",
        "                    'compliment_note', 'compliment_plain', 'compliment_cool',\n",
        "                    'compliment_funny', 'compliment_writer', 'compliment_photos',\n",
        "                    'funny', 'cool']\n",
        "    user_data = df_user[user_columns].rename(columns={'useful': 'user_useful',\n",
        "                                                      'funny': 'user_funny',\n",
        "                                                      'cool': 'user_cool'})\n",
        "\n",
        "    # Merge review and user data\n",
        "    df = df.merge(user_data, on='user_id', how='left')\n",
        "    return df\n",
        "\n",
        "df_hm = preprocess_reviews(df_hm, df_user)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hm_user = df_hm.drop_duplicates(subset='user_id').copy()\n",
        "\n",
        "# User-level columns\n",
        "user_cols = ['user_id', 'name', 'review_count', 'yelping_since', 'user_useful',\n",
        "             'elite', 'friends', 'fans', 'average_stars', 'user_funny', 'user_cool'] + \\\n",
        "            [col for col in df_hm.columns if col.startswith('compliment_')]\n",
        "\n",
        "hm_user = hm_user[user_cols]"
      ],
      "metadata": {
        "id": "ob9K-5_hblDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Relational social capital variables"
      ],
      "metadata": {
        "id": "IRR8V_u0cnJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert compliment columns to numeric and create compliment count\n",
        "compliment_cols = [col for col in hm_user.columns if col.startswith('compliment_')]\n",
        "hm_user[compliment_cols] = hm_user[compliment_cols].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "hm_user['compliment_num'] = hm_user[compliment_cols].sum(axis=1)"
      ],
      "metadata": {
        "id": "BBaMXQCTctsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import entropy\n",
        "\n",
        "# Calculate compliment diversity (Shannon entropy)\n",
        "def shannon_diversity(row):\n",
        "    values = row[compliment_cols].astype(float).values\n",
        "    total = values.sum()\n",
        "    if total == 0:\n",
        "        return 0\n",
        "    proportions = values / total\n",
        "    return entropy(proportions, base=np.e)\n",
        "\n",
        "hm_user['compliment_diversity'] = hm_user.apply(shannon_diversity, axis=1)"
      ],
      "metadata": {
        "id": "Xls9fna9crN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Social feedback (funny + cool votes)\n",
        "hm_user['user_funny'] = pd.to_numeric(hm_user['user_funny'], errors='coerce').fillna(0)\n",
        "hm_user['user_cool'] = pd.to_numeric(hm_user['user_cool'], errors='coerce').fillna(0)\n",
        "hm_user['social_feedback'] = hm_user['user_funny'] + hm_user['user_cool']"
      ],
      "metadata": {
        "id": "R6hFDzRec4WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Structural social capital variables"
      ],
      "metadata": {
        "id": "Q67X1QKfdJPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-igraph"
      ],
      "metadata": {
        "id": "rFfYJvm0ddj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hm_network = df_hm[['user_id', 'friends']].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# Build edge list\n",
        "edges = []\n",
        "nodes = set(hm_network['user_id'].astype(str))\n",
        "\n",
        "for row in hm_network.itertuples(index=False):\n",
        "    uid = str(row.user_id).strip()\n",
        "    if pd.isna(row.friends):\n",
        "        continue\n",
        "    friends = [f.strip() for f in row.friends.split(',') if f.strip()]\n",
        "    for f in friends:\n",
        "        if f != uid:\n",
        "            edges.append((uid, f))\n",
        "            nodes.add(f)\n",
        "\n",
        "# Unique undirected edges\n",
        "edge_set = set(tuple(sorted([a, b])) for a, b in edges)\n",
        "edges_df = pd.DataFrame(edge_set, columns=[\"node1\", \"node2\"])\n",
        "nodes_df = pd.DataFrame({\"user_id\": list(nodes)})"
      ],
      "metadata": {
        "id": "PY6og8gDdM5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build igraph graph\n",
        "import igraph as ig\n",
        "node_ids = nodes_df[\"user_id\"].astype(str).tolist()\n",
        "node_to_idx = {node_id: idx for idx, node_id in enumerate(node_ids)}\n",
        "\n",
        "edges = [\n",
        "    (node_to_idx[a.strip()], node_to_idx[b.strip()])\n",
        "    for a, b in edges_df[[\"node1\", \"node2\"]].values\n",
        "    if a.strip() in node_to_idx and b.strip() in node_to_idx and a.strip() != b.strip()\n",
        "]\n",
        "\n",
        "g = ig.Graph()\n",
        "g.add_vertices(len(node_ids))\n",
        "g.add_edges(edges)\n",
        "g.vs[\"name\"] = node_ids"
      ],
      "metadata": {
        "id": "cjsqqnCNdhXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Degree centrality\n",
        "\n",
        "nodes_df[\"degree\"] = g.degree()\n",
        "hm_user = hm_user.merge(nodes_df[[\"user_id\", \"degree\"]], on=\"user_id\", how=\"left\")"
      ],
      "metadata": {
        "id": "T99kFIZPdkoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pagerank centrality\n",
        "\n",
        "nodes_df[\"pagerank\"] = g.pagerank(damping=0.85, weights=None, directed=False)\n",
        "hm_user = hm_user.merge(nodes_df[[\"user_id\", \"pagerank\"]], on=\"user_id\", how=\"left\")"
      ],
      "metadata": {
        "id": "J1a-JDJEdmIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# k-core\n",
        "\n",
        "nodes_df[\"kcore\"] = g.coreness()\n",
        "hm_user = hm_user.merge(nodes_df[[\"user_id\", \"kcore\"]], on=\"user_id\", how=\"left\")"
      ],
      "metadata": {
        "id": "Q08CJZ1KdoVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Cognitive social capital variables"
      ],
      "metadata": {
        "id": "hIw_BzsVePY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob nltk==3.8.1 textstat"
      ],
      "metadata": {
        "id": "nCXuZhfMeSGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "1raienGCehf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stopwords: standard + platform name + sentiment words\n",
        "stop = set(stopwords.words('english'))\n",
        "stop.add(\"yelp\")\n",
        "sentiment_words = [\n",
        "    \"excellent\", \"love\", \"like\", \"awesome\", \"good\", \"great\", \"best\", \"perfect\",\n",
        "    \"nice\", \"super\", \"wish\", \"thanks\", \"thank\", \"lot\"\n",
        "]\n",
        "stop.update(sentiment_words)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "7PCOlfxpekqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function\n",
        "def clean_tokens(text):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "\n",
        "    # Lowercase and remove numbers\n",
        "    tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
        "\n",
        "    # POS tagging\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    # Remove adverbs (RB*), prepositions (IN), conjunctions (CC)\n",
        "    tagged = [(w, pos) for w, pos in tagged if not (\n",
        "        pos.startswith('RB') or pos in ['IN', 'CC']\n",
        "    )]\n",
        "\n",
        "    # Remove stopwords\n",
        "    tagged = [w for w, pos in tagged if w not in stop]\n",
        "\n",
        "    # Lemmatization (verbs vs nouns)\n",
        "    cleaned = []\n",
        "    for w, pos in pos_tag(tagged):\n",
        "        if pos.startswith('V'):\n",
        "            cleaned.append(lemmatizer.lemmatize(w, 'v'))\n",
        "        else:\n",
        "            cleaned.append(lemmatizer.lemmatize(w, 'n'))\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "df_hm['processed_text'] = df_hm['text'].apply(clean_tokens)"
      ],
      "metadata": {
        "id": "11meEVElenz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Narrativity\n",
        "\n",
        "pronouns = {\"i\",\"we\",\"you\",\"he\",\"she\",\"they\",\"me\",\"us\",\"him\",\"her\",\"them\"}\n",
        "time_words = {\"today\",\"yesterday\",\"tomorrow\",\"year\",\"month\",\"day\",\"week\",\"season\",\"hour\",\"minute\"}\n",
        "place_words = {\"home\",\"school\",\"office\",\"hospital\",\"restaurant\",\"city\",\"country\",\"room\",\"house\",\"street\",\"park\"}\n",
        "event_verbs = {\"go\",\"come\",\"say\",\"tell\",\"make\",\"do\",\"give\",\"take\",\"see\",\"meet\",\"leave\",\"arrive\"}\n",
        "\n",
        "def narrativity_score(text):\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return 0\n",
        "\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    if len(tokens) == 0:\n",
        "        return 0\n",
        "\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    count = 0\n",
        "    for word, pos in pos_tags:\n",
        "        if word in pronouns:\n",
        "            count += 1\n",
        "        elif word in time_words:\n",
        "            count += 1\n",
        "        elif word in place_words:\n",
        "            count += 1\n",
        "        elif word in event_verbs and pos.startswith(\"V\"):\n",
        "            count += 1\n",
        "\n",
        "    return count / len(tokens)\n",
        "\n",
        "df_hm['narrativity'] = df_hm['text'].fillna(\"\").apply(narrativity_score)"
      ],
      "metadata": {
        "id": "j9lWeQnd_W8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic Similarity\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n",
        "\n",
        "embeddings = model.encode(\n",
        "    df_hm['processed_text'].fillna(\"\").tolist(),\n",
        "    batch_size=128,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "corpus_mean = np.mean(embeddings, axis=0).reshape(1, -1)\n",
        "\n",
        "df_hm['semantic_similarity'] = cosine_similarity(embeddings, corpus_mean).flatten()"
      ],
      "metadata": {
        "id": "gfhExW49itb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concept overlap\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(df_hm['processed_text'].fillna(\"\"))\n",
        "\n",
        "corpus_mean = X_tfidf.mean(axis=0).A\n",
        "\n",
        "df_hm['concept_overlap'] = cosine_similarity(X_tfidf, corpus_mean).flatten()"
      ],
      "metadata": {
        "id": "pr33GBORKKyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Moderator"
      ],
      "metadata": {
        "id": "KGs-nO1mc7D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Elite years count\n",
        "\n",
        "def count_elite_years(val):\n",
        "    if pd.isna(val) or str(val).strip() == '':\n",
        "        return 0\n",
        "    return len(str(val).split(','))\n",
        "hm_user['elite_count'] = hm_user['elite'].apply(count_elite_years)"
      ],
      "metadata": {
        "id": "VB6qaMI9c-PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Aggregate dataset"
      ],
      "metadata": {
        "id": "PPFbjcSvk4T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables to aggregate from df_hm\n",
        "keep_cols = [\n",
        "    \"user_id\",\"semantic_similarity\", \"narrativity\", \"concept_overlap\"\n",
        "]\n",
        "\n",
        "# Aggregate mean by user_id\n",
        "agg_df = df_hm[keep_cols].groupby(\"user_id\").mean(numeric_only=True).reset_index()\n",
        "\n",
        "# Merge with existing hm_user (which already has relational variables)\n",
        "data = hm_user.merge(agg_df, on=\"user_id\", how=\"left\")"
      ],
      "metadata": {
        "id": "0ZaUhJ5ok6G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "MAjwGXZZmg-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# Save final dataset\n",
        "data.to_csv(\"data.csv\", index=False)\n",
        "\n",
        "files.download(\"data.csv\")"
      ],
      "metadata": {
        "id": "W59wg8hSjBfA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}