{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaKFVl+rOt5WJDwmJLQeUA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisa11323/CSR_yelp/blob/main/ECRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz6PucmFzU8t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Prepare dataset"
      ],
      "metadata": {
        "id": "JDFKoHs9t51f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download open dataset\n",
        "# https://business.yelp.com/data/resources/open-dataset/"
      ],
      "metadata": {
        "id": "dFUGfs8nuhpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload files manually from local machine\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "uWHlB-TeWb3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert JSON to CSV\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load Yelp dataset JSON files\n",
        "df_review = pd.read_json(\"yelp_academic_dataset_review.json\", lines=True)\n",
        "df_user = pd.read_json(\"yelp_academic_dataset_user.json\", lines=True)\n",
        "df_biz = pd.read_json(\"yelp_academic_dataset_business.json\", lines=True)"
      ],
      "metadata": {
        "id": "PXdz0KCfXEvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Sampling Dataset"
      ],
      "metadata": {
        "id": "6gHvAo_TWSNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorize businesses\n",
        "\n",
        "import re\n",
        "\n",
        "healthcare_cats = [\n",
        "    'Health & Medical', 'Medical Centers', 'Doctors', 'Dentists', 'General Dentistry',\n",
        "    'Cosmetic Dentists', 'Oral Surgeons', 'Endodontists', 'Orthodontists', 'Pediatric Dentists',\n",
        "    'Veterinarians', 'Chiropractors', 'Physical Therapy', 'Urgent Care'\n",
        "    ]\n",
        "\n",
        "repair_cats = [\n",
        "    'Auto Repair', 'Heating & Air Conditioning/HVAC', 'Plumbing', 'Appliances & Repair',\n",
        "    'Auto Glass Services', 'Transmission Repair', 'Water Heater Installation/Repair'\n",
        "    ]\n",
        "\n",
        "professional_cats = [\n",
        "    'Financial Services', 'Banks & Credit Unions', 'Real Estate Services',\n",
        "    'Property Management', 'Real Estate Agents', 'Notaries'\n",
        "    ]\n",
        "\n",
        "def norm_token(x):\n",
        "  if pd.isna(x):\n",
        "    return \"\"\n",
        "  return str(x).strip().lower()\n",
        "\n",
        "health_set = {norm_token(c) for c in healthcare_cats}\n",
        "repair_set = {norm_token(c) for c in repair_cats}\n",
        "prof_set = {norm_token(c) for c in professional_cats}\n",
        "\n",
        "df_biz = df_biz.copy()\n",
        "df_biz[\"categories_token\"] = (\n",
        "    df_biz[\"categories\"]\n",
        "    .fillna(\"\")\n",
        "    .astype(str)\n",
        "    .str.split(\",\")\n",
        "    .apply(lambda xs: [norm_token(t) for t in xs if norm_token(t)])\n",
        "    )\n",
        "\n",
        "df_biz[\"is_healthcare\"] = df_biz[\"categories_token\"].apply(lambda toks: int(any(t in health_set for t in toks)))\n",
        "df_biz[\"is_repair\"] = df_biz[\"categories_token\"].apply(lambda toks: int(any(t in repair_set for t in toks)))\n",
        "df_biz[\"is_professional\"] = df_biz[\"categories_token\"].apply(lambda toks: int(any(t in prof_set for t in toks)))\n",
        "biz_healthcare = df_biz.loc[df_biz[\"is_healthcare\"] == 1].drop_duplicates(subset=\"business_id\").copy()\n",
        "biz_repair = df_biz.loc[df_biz[\"is_repair\"] == 1].drop_duplicates(subset=\"business_id\").copy()\n",
        "biz_professional = df_biz.loc[df_biz[\"is_professional\"] == 1].drop_duplicates(subset=\"business_id\").copy()\n",
        "\n",
        "mask_credence = (df_biz[\"is_healthcare\"] == 1) | (df_biz[\"is_repair\"] == 1) | (df_biz[\"is_professional\"] == 1)\n",
        "biz_credence = df_biz.loc[mask_credence].drop_duplicates(subset=\"business_id\").copy()\n",
        "\n",
        "biz_credence['postal_code'] = biz_credence['postal_code'].astype(str).str.strip()\n",
        "mask_postal = biz_credence['postal_code'].str.fullmatch(r'\\d{5}')\n",
        "biz_credence.loc[~mask_postal, 'postal_code'] = pd.NA\n",
        "biz_credence = biz_credence.dropna(subset=['categories', 'business_id','postal_code'])\n",
        "\n",
        "print(\"biz_credence:\", biz_credence.shape)"
      ],
      "metadata": {
        "id": "DlfJppSer9ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract reviews of credence goods and services\n",
        "\n",
        "df_review[\"business_id\"] = df_review[\"business_id\"].astype(str)\n",
        "biz_credence[\"business_id\"] = biz_credence[\"business_id\"].astype(str)\n",
        "\n",
        "cred_biz_ids = biz_credence[\"business_id\"].dropna().astype(str).unique()\n",
        "rev_credence = df_review[df_review[\"business_id\"].isin(cred_biz_ids)].copy()\n",
        "\n",
        "rev_credence['date'] = pd.to_datetime(rev_credence['date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "rev_credence['text'] = rev_credence['text'].astype(str).str.strip()\n",
        "rev_credence.loc[rev_credence['text'] == '', 'text'] = pd.NA\n",
        "\n",
        "print(\"rev_credence:\", rev_credence.shape)"
      ],
      "metadata": {
        "id": "SvVdroXa5djb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify reviewers of credence goods and services\n",
        "\n",
        "rev_credence[\"user_id\"] = rev_credence[\"user_id\"].astype(str)\n",
        "df_user[\"user_id\"] = df_user[\"user_id\"].astype(str)\n",
        "\n",
        "cred_user_ids = rev_credence[\"user_id\"].dropna().astype(str).unique()\n",
        "user_credence = df_user[df_user[\"user_id\"].isin(cred_user_ids)].copy()\n",
        "\n",
        "user_credence['yelping_since'] = pd.to_datetime(user_credence['yelping_since'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "\n",
        "user_credence[\"name\"] = user_credence[\"name\"].astype(str)\n",
        "user_credence[\"name\"] = user_credence[\"name\"].str.strip()\n",
        "user_credence.loc[user_credence[\"name\"].isin([\"\", \"nan\", \"None\", \"NA\", \"N/A\"]), \"name\"] = np.nan\n",
        "\n",
        "exclude_cols = [\"elite\", \"friends\"]\n",
        "cols_to_check = [c for c in user_credence.columns if c not in exclude_cols]\n",
        "user_credence = user_credence.dropna(subset=cols_to_check, how=\"any\").copy()\n",
        "\n",
        "print(\"user_credence\", user_credence.shape)"
      ],
      "metadata": {
        "id": "0YTgZOAyM_qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop users with missing data\n",
        "\n",
        "rev_credence[\"user_id\"] = rev_credence[\"user_id\"].astype(str)\n",
        "user_credence[\"user_id\"] = user_credence[\"user_id\"].astype(str)\n",
        "\n",
        "rev_user_ids = pd.Index(rev_credence[\"user_id\"].dropna().unique())\n",
        "user_ids = pd.Index(user_credence[\"user_id\"].dropna().unique())\n",
        "\n",
        "missing_in_user = rev_user_ids.difference(user_ids)\n",
        "\n",
        "rev_credence = rev_credence[~rev_credence[\"user_id\"].isin(missing_in_user)].copy()\n",
        "\n",
        "print(\"rev_credence\", rev_credence.shape)"
      ],
      "metadata": {
        "id": "zsugv1Mr5tBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a prefix and aggregate columns\n",
        "\n",
        "def preprocess_reviews(rev_credence, user_credence, biz_credence):\n",
        "  rev_credence = rev_credence.rename(columns={\n",
        "      'useful': 'rev_useful',\n",
        "      'funny': 'rev_funny',\n",
        "      'cool': 'rev_cool'\n",
        "      })\n",
        "\n",
        "  user_columns = [\n",
        "      'user_id', 'name', 'review_count', 'yelping_since', 'useful', 'elite',\n",
        "      'friends', 'fans', 'average_stars', 'compliment_hot', 'compliment_more',\n",
        "      'compliment_profile', 'compliment_cute', 'compliment_list',\n",
        "      'compliment_note', 'compliment_plain', 'compliment_cool',\n",
        "      'compliment_funny', 'compliment_writer', 'compliment_photos',\n",
        "      'funny', 'cool'\n",
        "      ]\n",
        "  user_columns_exist = [c for c in user_columns if c in user_credence.columns]\n",
        "  user_data = user_credence[user_columns].rename(columns={\n",
        "      'useful': 'user_useful',\n",
        "      'funny': 'user_funny',\n",
        "      'cool': 'user_cool'\n",
        "      })\n",
        "\n",
        "  biz_columns = [\n",
        "      'business_id', 'name', 'city', 'state', 'postal_code', 'stars',\n",
        "      \"categories\", \"categories_token\", \"is_healthcare\", \"is_repair\", \"is_professional\"\n",
        "\n",
        "      ]\n",
        "  biz_columns_exist = [c for c in biz_columns if c in biz_credence.columns]\n",
        "  biz_data = biz_credence[biz_columns].rename(columns={\n",
        "      'name': 'biz_name',\n",
        "      'stars': 'biz_stars'\n",
        "      })\n",
        "  out = rev_credence.merge(user_data, on=\"user_id\", how=\"left\")\n",
        "  out = out.merge(biz_data, on=\"business_id\", how=\"left\")\n",
        "  return out\n",
        "\n",
        "rev_credence = preprocess_reviews(rev_credence, user_credence, biz_credence)\n",
        "\n",
        "print(rev_credence.shape)"
      ],
      "metadata": {
        "id": "3ePJ8rOGsraV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_credence.rename(columns={\n",
        "    \"useful\": \"user_useful\",\n",
        "    \"funny\": \"user_funny\",\n",
        "    \"cool\": \"user_cool\"\n",
        "}, inplace=True)\n",
        "\n",
        "biz_credence.rename(columns={\n",
        "    \"name\": \"biz_name\",\n",
        "    \"stars\": \"biz_stars\"\n",
        "}, inplace=True)"
      ],
      "metadata": {
        "id": "-OmfpEzEsP4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"rev_credence:\", rev_credence.shape)\n",
        "print(\"biz_credence:\", biz_credence.shape)\n",
        "print(\"user_credence\", user_credence.shape)"
      ],
      "metadata": {
        "id": "KJsG_pg_S9I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Relational social capital variables"
      ],
      "metadata": {
        "id": "IRR8V_u0cnJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compliment count\n",
        "def add_compliment_num(df):\n",
        "  df = df.copy()\n",
        "  compliment_cols = [col for col in df.columns if col.startswith('compliment_')]\n",
        "  df[compliment_cols] = df[compliment_cols].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "  df['compliment_num'] = df[compliment_cols].sum(axis=1)\n",
        "  return df\n",
        "\n",
        "user_credence = add_compliment_num(user_credence)"
      ],
      "metadata": {
        "id": "BBaMXQCTctsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compliment diversity (Shannon entropy)\n",
        "\n",
        "from scipy.stats import entropy\n",
        "import numpy as np\n",
        "\n",
        "def add_compliment_diversity(df):\n",
        "  df = df.copy()\n",
        "  compliment_cols = [col for col in df.columns if col.startswith('compliment_')]\n",
        "  df[compliment_cols] = df[compliment_cols].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "\n",
        "  def shannon_diversity(row):\n",
        "    values = row[compliment_cols].astype(float).values\n",
        "    total = values.sum()\n",
        "    if total == 0:\n",
        "      return 0.0\n",
        "    proportions = values / total\n",
        "    return entropy(proportions, base=np.e)\n",
        "\n",
        "  df['compliment_diversity'] = df.apply(shannon_diversity, axis=1)\n",
        "  return df\n",
        "\n",
        "user_credence = add_compliment_diversity(user_credence)"
      ],
      "metadata": {
        "id": "Xls9fna9crN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Social feedback\n",
        "\n",
        "def add_social_feedback(df):\n",
        "  df = df.copy()\n",
        "  df[\"social_feedback\"] = df[\"user_funny\"].astype(\"int64\") + df[\"user_cool\"].astype(\"int64\")\n",
        "  return df\n",
        "\n",
        "user_credence = add_social_feedback(user_credence)"
      ],
      "metadata": {
        "id": "R6hFDzRec4WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Structural social capital variables"
      ],
      "metadata": {
        "id": "Q67X1QKfdJPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-igraph"
      ],
      "metadata": {
        "id": "rFfYJvm0ddj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_user_network(df):\n",
        "  df_net = df[['user_id', 'friends']].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "  edges = []\n",
        "  nodes = set(df_net['user_id'].astype(str))\n",
        "\n",
        "  for row in df_net.itertuples(index=False):\n",
        "    uid = str(row.user_id).strip()\n",
        "    friends_val = row.friends\n",
        "    if pd.isna(friends_val):\n",
        "      continue\n",
        "    friends = [f.strip() for f in str(friends_val).split(',') if f.strip()]\n",
        "    for f in friends:\n",
        "      if f != uid:\n",
        "        edges.append(tuple(sorted([uid, f])))\n",
        "        nodes.add(f)\n",
        "  if edges:\n",
        "    edge_set = set(edges)\n",
        "    edges_df = pd.DataFrame(list(edge_set), columns=[\"node1\", \"node2\"])\n",
        "  else:\n",
        "    edges_df = pd.DataFrame(columns=[\"node1\", \"node2\"])\n",
        "\n",
        "  nodes_df = pd.DataFrame({\"user_id\": list(nodes)})\n",
        "\n",
        "  return edges_df, nodes_df\n",
        "\n",
        "edges_credence, nodes_credence = build_user_network(user_credence)"
      ],
      "metadata": {
        "id": "PY6og8gDdM5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import igraph as ig\n",
        "\n",
        "def build_graph(nodes_df, edges_df):\n",
        "  node_ids = nodes_df[\"user_id\"].astype(str).tolist()\n",
        "  node_to_idx = {node_id: idx for idx, node_id in enumerate(node_ids)}\n",
        "\n",
        "  edge_list = [\n",
        "      (node_to_idx[a.strip()], node_to_idx[b.strip()])\n",
        "      for a, b in edges_df[[\"node1\", \"node2\"]].values\n",
        "      if isinstance(a, str) and isinstance(b, str)\n",
        "      and a.strip() in node_to_idx and b.strip() in node_to_idx\n",
        "      and a.strip() != b.strip()\n",
        "      ]\n",
        "\n",
        "  g = ig.Graph()\n",
        "  g.add_vertices(len(node_ids))\n",
        "  if edge_list:\n",
        "    g.add_edges(edge_list)\n",
        "  g.vs[\"name\"] = node_ids\n",
        "  return g\n",
        "\n",
        "g_credence = build_graph(nodes_credence, edges_credence)"
      ],
      "metadata": {
        "id": "cjsqqnCNdhXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Degree centrality\n",
        "\n",
        "def add_degree_centrality(user_df, nodes_df, g):\n",
        "  nodes_tmp = nodes_df.copy()\n",
        "  nodes_tmp[\"degree\"] = g.degree()\n",
        "  user_df = user_df.merge(nodes_tmp[[\"user_id\", \"degree\"]], on=\"user_id\", how=\"left\")\n",
        "  return user_df\n",
        "\n",
        "user_credence = add_degree_centrality(user_credence, nodes_credence, g_credence)"
      ],
      "metadata": {
        "id": "T99kFIZPdkoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pagerank centrality\n",
        "\n",
        "def add_pagerank_centrality(user_df, nodes_df, g):\n",
        "  nodes_tmp = nodes_df.copy()\n",
        "  nodes_tmp[\"pagerank\"] = g.pagerank(damping=0.85, weights=None, directed=False)\n",
        "  user_df = user_df.merge(nodes_tmp[[\"user_id\", \"pagerank\"]], on=\"user_id\", how=\"left\")\n",
        "  return user_df\n",
        "\n",
        "user_credence = add_pagerank_centrality(user_credence, nodes_credence, g_credence)"
      ],
      "metadata": {
        "id": "J1a-JDJEdmIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# k-core centrality\n",
        "\n",
        "def add_kcore_centrality(user_df, nodes_df, g):\n",
        "  nodes_tmp = nodes_df.copy()\n",
        "  nodes_tmp[\"kcore\"] = g.coreness()\n",
        "  user_df = user_df.merge(nodes_tmp[[\"user_id\", \"kcore\"]], on=\"user_id\", how=\"left\")\n",
        "  return user_df\n",
        "\n",
        "user_credence = add_kcore_centrality(user_credence, nodes_credence, g_credence)"
      ],
      "metadata": {
        "id": "Q08CJZ1KdoVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Cognitive social capital variables"
      ],
      "metadata": {
        "id": "hIw_BzsVePY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob nltk==3.8.1 textstat"
      ],
      "metadata": {
        "id": "nCXuZhfMeSGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "1raienGCehf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords: standard + platform name + sentiment words\n",
        "stop = set(stopwords.words('english'))\n",
        "stop.add(\"yelp\")\n",
        "sentiment_words = [\n",
        "    \"excellent\", \"love\", \"like\", \"awesome\", \"good\", \"great\", \"best\", \"perfect\",\n",
        "    \"nice\", \"super\", \"wish\", \"thanks\", \"thank\", \"lot\"\n",
        "    ]\n",
        "stop.update(sentiment_words)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "7PCOlfxpekqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tokens(text):\n",
        "  if pd.isna(text):\n",
        "    return []\n",
        "\n",
        "  tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
        "  tagged = pos_tag(tokens)\n",
        "\n",
        "  tagged = [(w, pos) for w, pos in tagged if not (\n",
        "      pos.startswith('RB') or pos in ['IN', 'CC']\n",
        "      )]\n",
        "\n",
        "  tagged = [w for w, pos in tagged if w not in stop]\n",
        "\n",
        "  cleaned = []\n",
        "  for w, pos in pos_tag(tagged):\n",
        "    if pos.startswith('V'):\n",
        "      cleaned.append(lemmatizer.lemmatize(w, 'v'))\n",
        "    else:\n",
        "      cleaned.append(lemmatizer.lemmatize(w, 'n'))\n",
        "\n",
        "  return cleaned\n",
        "\n",
        "rev_credence['processed_text'] = rev_credence['text'].apply(clean_tokens)"
      ],
      "metadata": {
        "id": "11meEVElenz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Narrativity\n",
        "\n",
        "pronouns = {\"i\",\"we\",\"you\",\"he\",\"she\",\"they\",\"me\",\"us\",\"him\",\"her\",\"them\"}\n",
        "time_words = {\"today\",\"yesterday\",\"tomorrow\",\"year\",\"month\",\"day\",\"week\",\"season\",\"hour\",\"minute\"}\n",
        "place_words = {\"home\",\"school\",\"office\",\"hospital\",\"restaurant\",\"city\",\"country\",\"room\",\"house\",\"street\",\"park\"}\n",
        "event_verbs = {\"go\",\"come\",\"say\",\"tell\",\"make\",\"do\",\"give\",\"take\",\"see\",\"meet\",\"leave\",\"arrive\"}\n",
        "\n",
        "def narrativity_score(text):\n",
        "  if not isinstance(text, str) or text.strip() == \"\":\n",
        "    return 0\n",
        "\n",
        "  tokens = nltk.word_tokenize(text.lower())\n",
        "  if len(tokens) == 0:\n",
        "    return 0\n",
        "\n",
        "  pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "  count = 0\n",
        "  for word, pos in pos_tags:\n",
        "    if word in pronouns:\n",
        "      count += 1\n",
        "    elif word in time_words:\n",
        "      count += 1\n",
        "    elif word in place_words:\n",
        "      count += 1\n",
        "    elif word in event_verbs and pos.startswith(\"V\"):\n",
        "      count += 1\n",
        "\n",
        "  return count / len(tokens)\n",
        "\n",
        "def add_narrativity(df):\n",
        "  df['narrativity'] = df['text'].fillna(\"\").apply(narrativity_score)\n",
        "  return df\n",
        "\n",
        "rev_credence = add_narrativity(rev_credence)"
      ],
      "metadata": {
        "id": "j9lWeQnd_W8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic similarity\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def sbert_semantic_similarity_processed_text(\n",
        "    df,\n",
        "    model_name=\"paraphrase-MiniLM-L3-v2\",\n",
        "    text_col=\"processed_text\",\n",
        "    out_col=\"semantic_similarity\",\n",
        "    batch_size=128,\n",
        "    dtype=np.float32\n",
        "):\n",
        "    out = df.copy()\n",
        "    texts = out[text_col].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    emb = model.encode(\n",
        "        texts,\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True\n",
        "    ).astype(dtype, copy=False)\n",
        "\n",
        "    global_centroid = np.asarray(emb.mean(axis=0, keepdims=True))\n",
        "    sims = np.empty(len(texts), dtype=np.float32)\n",
        "\n",
        "    step = max(2048, batch_size)\n",
        "    for i in range(0, len(texts), step):\n",
        "        Xb = emb[i:i + step, :]\n",
        "        sims[i:i + Xb.shape[0]] = cosine_similarity(Xb, global_centroid).ravel().astype(np.float32, copy=False)\n",
        "\n",
        "    out[out_col] = sims\n",
        "    return out\n",
        "\n",
        "rev_credence = sbert_semantic_similarity_processed_text(\n",
        "    rev_credence,\n",
        "    model_name=\"paraphrase-MiniLM-L3-v2\",\n",
        "    text_col=\"processed_text\",\n",
        "    out_col=\"semantic_similarity\",\n",
        "    batch_size=128\n",
        ")"
      ],
      "metadata": {
        "id": "gfhExW49itb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concept overlap\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def to_text(x):\n",
        "    if isinstance(x, list):\n",
        "        return \" \".join(map(str, x))\n",
        "    if isinstance(x, str):\n",
        "        return x\n",
        "    return \"\"\n",
        "\n",
        "def add_concept_overlap(df, text_col=\"processed_text\", max_features=5000, out_col=\"concept_overlap\"):\n",
        "    out = df.copy()\n",
        "\n",
        "    texts = out[text_col].apply(to_text).fillna(\"\").astype(str).tolist()\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "\n",
        "    out[\"concept_overlap_healthcare_tmp\"] = np.nan\n",
        "    out[\"concept_overlap_repair_tmp\"] = np.nan\n",
        "    out[\"concept_overlap_professional_tmp\"] = np.nan\n",
        "\n",
        "    sector_specs = {\n",
        "        \"concept_overlap_healthcare_tmp\": (\"is_healthcare\", 1),\n",
        "        \"concept_overlap_repair_tmp\": (\"is_repair\", 1),\n",
        "        \"concept_overlap_professional_tmp\": (\"is_professional\", 1),\n",
        "    }\n",
        "\n",
        "    for tmp_col, (flag_col, flag_val) in sector_specs.items():\n",
        "        mask = (pd.to_numeric(out[flag_col], errors=\"coerce\") == flag_val).to_numpy()\n",
        "        idx = np.where(mask)[0]\n",
        "        if len(idx) == 0:\n",
        "            continue\n",
        "        Xs = X[idx]\n",
        "        centroid = np.asarray(Xs.mean(axis=0)).ravel()\n",
        "        out.iloc[idx, out.columns.get_loc(tmp_col)] = cosine_similarity(Xs, centroid.reshape(1, -1)).ravel()\n",
        "\n",
        "    tmp_cols = [\"concept_overlap_healthcare_tmp\", \"concept_overlap_repair_tmp\", \"concept_overlap_professional_tmp\"]\n",
        "    out[out_col] = out[tmp_cols].mean(axis=1, skipna=True)\n",
        "\n",
        "    out = out.drop(columns=tmp_cols, errors=\"ignore\")\n",
        "    return out\n",
        "\n",
        "rev_credence = add_concept_overlap(\n",
        "    rev_credence,\n",
        "    text_col=\"processed_text\",\n",
        "    max_features=5000,\n",
        "    out_col=\"concept_overlap\"\n",
        ")"
      ],
      "metadata": {
        "id": "pr33GBORKKyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Review length (Information load)\n",
        "\n",
        "def add_review_length(df):\n",
        "  df['review_length'] = df['text'].fillna(\"\").str.split().str.len()\n",
        "  return df\n",
        "\n",
        "rev_credence = add_review_length(rev_credence)"
      ],
      "metadata": {
        "id": "gBM-iBwVWSl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Moderator"
      ],
      "metadata": {
        "id": "KGs-nO1mc7D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Platform-driven recognition (elite count)\n",
        "def count_elite_years(val):\n",
        "  if pd.isna(val) or str(val).strip() == '':\n",
        "    return 0\n",
        "  return len(str(val).split(','))\n",
        "\n",
        "def add_elite_count(df):\n",
        "  df['elite_count'] = df['elite'].apply(count_elite_years)\n",
        "  return df\n",
        "\n",
        "user_credence = add_elite_count(user_credence)"
      ],
      "metadata": {
        "id": "VB6qaMI9c-PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Control variables"
      ],
      "metadata": {
        "id": "mVEiqwa-Wiyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference date: most recent review date (time-unit normalized)\n",
        "reference_date = rev_credence['date'].max().normalize()\n",
        "print(reference_date)  # 2022-01-19 00:00:00\n",
        "\n",
        "def add_tenure(df, reference_date):\n",
        "  df['tenure'] = (reference_date - df['yelping_since']).dt.days / 365.25\n",
        "  return df\n",
        "\n",
        "user_credence = add_tenure(user_credence, reference_date)"
      ],
      "metadata": {
        "id": "f03hdrjcWiDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Geographic breadth\n",
        "\n",
        "user_city = rev_credence.groupby(\"user_id\", as_index=False).agg(n_city=(\"city\", pd.Series.nunique))\n",
        "\n",
        "rev_credence = rev_credence.merge(user_city, on=\"user_id\", how=\"left\")"
      ],
      "metadata": {
        "id": "YfMtbtEGXX0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Aggregate dataset"
      ],
      "metadata": {
        "id": "PPFbjcSvk4T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables to aggregate in rev_credence\n",
        "keep_cols = [\n",
        "    \"user_id\", \"narrativity\", \"semantic_similarity\", \"concept_overlap\", \"review_length\", \"n_city\"\n",
        "]\n",
        "\n",
        "# Compute user-level means (grouped by user_id)\n",
        "def aggregate_user_level(user_df, review_df):\n",
        "  agg_df = review_df[keep_cols].groupby(\"user_id\").mean(numeric_only=True).reset_index()\n",
        "  merged = user_df.merge(agg_df, on=\"user_id\", how=\"left\")\n",
        "  return merged\n",
        "\n",
        "data_credence = aggregate_user_level(user_credence, rev_credence)"
      ],
      "metadata": {
        "id": "0ZaUhJ5ok6G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# Save final dataset\n",
        "data_credence.to_csv(\"data_credence.csv\", index=False)\n",
        "\n",
        "files.download(\"data_credence.csv\")"
      ],
      "metadata": {
        "id": "W59wg8hSjBfA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}